
# ğŸ“„ QA with Documents (Text) using Gemini + LlamaIndex

This project enables **question-answering (QA)** on uploaded documents using Google **Gemini Pro LLM**, **Gemini Embeddings**, and **LlamaIndex**. A user-friendly interface is provided via **Streamlit** to allow natural language querying of any uploaded PDF, TXT, or Markdown file.

---

## ğŸš€ Features

- âœ… Upload PDF, TXT, or MD files via Streamlit UI  
- âœ… Extracts and indexes content using `LlamaIndex`  
- âœ… Embeds text using `GeminiEmbedding`  
- âœ… Answers questions with Google's `Gemini-Pro` LLM  
- âœ… Logs all key steps and errors for debugging  
- âœ… Modular codebase with `data_ingestion`, `embedding`, `model_api`, and `app` separation  

---

## ğŸ› ï¸ Tech Stack

- [LlamaIndex](https://www.llamaindex.ai/)
- [Google Generative AI SDK](https://ai.google.dev/)
- [Streamlit](https://streamlit.io/)
- [Python 3.10+](https://www.python.org/)
- Modular Python architecture using custom `exception` and `logger` modules

---

## ğŸ“ Project Structure

```
QAWithPDF/
â”‚
â”œâ”€â”€ App.py                  # Streamlit app
â”œâ”€â”€ data_ingestion.py       # Document loading
â”œâ”€â”€ embedding.py            # Embedding + indexing
â”œâ”€â”€ model_api.py            # Gemini model loader
â”œâ”€â”€ exception.py            # Custom exception handling
â”œâ”€â”€ logger.py               # Logging configuration
â”œâ”€â”€ __init__.py             # Optional package marker
â”œâ”€â”€ requirements.txt        # Required dependencies
â”œâ”€â”€ .env                    # Contains your Google API Key
â””â”€â”€ Data/                   # Uploaded files directory (auto-created)
```

---

## âš™ï¸ Setup Instructions

### 1. âœ… Clone the repository

```bash
git clone https://github.com/your-username/QAWithPDF.git
cd QAWithPDF
```

### 2. âœ… Create and activate virtual environment

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. âœ… Install dependencies

```bash
pip install -r requirements.txt
```

### 4. âœ… Set your API key in a `.env` file

Create a `.env` file in the root with:

```
GOOGLE_API_KEY=your_google_genai_api_key
```

### 5. âœ… Run the app

```bash
streamlit run App.py
```

---

## ğŸ“¦ requirements.txt

```txt
llama-index>=0.10.28
google-generativeai
streamlit
python-dotenv
ipython
tqdm
```

---

## ğŸ’¡ Example Questions You Can Ask

- â€œWhat is this document about?â€
- â€œWhat are the differences between supervised and unsupervised learning?â€
- â€œSummarize this file in 3 bullet points.â€

---

## ğŸ›¡ï¸ Notes

- `ServiceContext` is deprecated â€“ we use `Settings` from `llama_index.core.settings`.
- Works with Googleâ€™s `gemini-pro` and `embedding-001` models.

---

## ğŸ™Œ Acknowledgments
- [Suuny Savita] (https://www.youtube.com/watch?v=hqJxgbxczOo&list=PLQxDHpeGU14Blorx3Ps1eZJ4XvKET1_vx)
- [LlamaIndex Documentation](https://docs.llamaindex.ai/)
- [Google Gemini SDK](https://ai.google.dev/)
- [Streamlit](https://streamlit.io/)
